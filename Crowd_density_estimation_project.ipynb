{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amrutha0610/Crowd_Density_Estimation/blob/main/Crowd_density_estimation_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crowd Counting and Density Estimation"
      ],
      "metadata": {
        "id": "7WWUFDWUoful"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSRNet model respository cloning from github"
      ],
      "metadata": {
        "id": "TEtqOShdVuCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup\n",
        "\n",
        "This section handles mounting Google Drive to access files, changing the current directory to the project's working directory, cloning the necessary GitHub repository for the CSRNet model, and installing the required Python packages to run the code."
      ],
      "metadata": {
        "id": "VdZObCd7f-Pf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gyzc7PKQOrUa"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change to working directory\n",
        "%cd /content/drive/MyDrive/crowd_density\n",
        "# check files\n",
        "!ls\n",
        "\n",
        "# Clone CSRNet repository\n",
        "!git clone https://github.com/leeyeehoo/CSRNet-pytorch.git\n",
        "%cd CSRNet-pytorch\n",
        "\n",
        "# Install required packages\n",
        "!pip install h5py opencv-python scipy pillow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset and Model Paths\n",
        "\n",
        "This cell defines the file paths for the training and testing datasets, as well as the paths for saving the trained models in Google Drive."
      ],
      "metadata": {
        "id": "a4owizxpgsNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset directories in Google Drive\n",
        "train_image_path = '/content/drive/MyDrive/crowd_density/ShanghaiTech_Crowd_Counting_Dataset/part_B_final/train_data/images'\n",
        "train_density_path = '/content/drive/MyDrive/crowd_density/ShanghaiTech_Crowd_Counting_Dataset/part_B_final/train_data/ground_truth'\n",
        "\n",
        "test_image_path = '/content/drive/MyDrive/crowd_density/ShanghaiTech_Crowd_Counting_Dataset/part_B_final/test_data/images'\n",
        "test_density_path = '/content/drive/MyDrive/crowd_density/ShanghaiTech_Crowd_Counting_Dataset/part_B_final/test_data/ground_truth'\n",
        "\n",
        "# Model paths\n",
        "#partA_model_path = '/content/drive/MyDrive/crowd_density/PartAmodel_best.pth.tar'\n",
        "#partB_model_path = '/content/drive/MyDrive/crowd_density/partBmodel_best.pth.tar'\n",
        "\n"
      ],
      "metadata": {
        "id": "QbH8fUPwPPIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically replace all \"xrange\" with \"range\" in model.py\n",
        "!sed -i 's/xrange/range/g' /content/drive/MyDrive/crowd_density/CSRNet-pytorch/model.py"
      ],
      "metadata": {
        "id": "7kPYPkIdcO6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch dataset pipeline\n",
        "CrowdDataset class: Loads and processes crowd counting images and their corresponding density maps. Supports random horizontal flipping as data augmentation.\n",
        "\n",
        "custom_collate_fn function: Ensures all images and density maps in a batch are resized to the same dimensions for proper batch training.\n",
        "\n",
        "DataLoaders: Prepares batches of training and test data with correct transformations and collate functions."
      ],
      "metadata": {
        "id": "J4gYzyEfhayL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import random\n",
        "\n",
        "class CrowdDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for crowd counting.\n",
        "    Loads images and corresponding density maps, applies preprocessing and optional data augmentation.\n",
        "\n",
        "    Args:\n",
        "        image_dir (str): Directory path containing image files.\n",
        "        density_dir (str): Directory path containing corresponding density map (.mat) files.\n",
        "        transform (callable, optional): Transformations to apply to input images.\n",
        "        augment (bool, optional): Whether to apply data augmentation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Tensor, Tensor]: Transformed image and corresponding density map tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_dir: str, density_dir: str, transform=None, augment: bool = False):\n",
        "        self.image_dir = image_dir\n",
        "        self.density_dir = density_dir\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the total number of images in the dataset.\"\"\"\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"Fetches the image and corresponding density map at the specified index.\"\"\"\n",
        "\n",
        "        image_file = self.image_files[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_file)\n",
        "\n",
        "        # Load corresponding ground truth density map (.mat file)\n",
        "        density_file = 'GT_' + image_file.replace('.jpg', '.mat')\n",
        "        density_path = os.path.join(self.density_dir, density_file)\n",
        "\n",
        "        # Load image and convert to RGB\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        width, height = image.size\n",
        "\n",
        "        # Load ground truth points from the .mat file\n",
        "        mat = loadmat(density_path)\n",
        "        points = mat[\"image_info\"][0, 0][0, 0][0]\n",
        "\n",
        "        # Create a blank density map and place 1 at each annotated point\n",
        "        density_map = np.zeros((height, width), dtype=np.float32)\n",
        "        for point in points:\n",
        "            x = min(int(point[0]), width - 1)\n",
        "            y = min(int(point[1]), height - 1)\n",
        "            density_map[y, x] = 1\n",
        "\n",
        "        # Apply Gaussian smoothing to convert points to a density map\n",
        "        density_map = gaussian_filter(density_map, sigma=15)\n",
        "\n",
        "        # Convert density map to PIL Image for possible transformations\n",
        "        density_map_img = Image.fromarray(density_map)\n",
        "\n",
        "        if self.augment:\n",
        "            # Apply random horizontal flip\n",
        "            if random.random() > 0.5:\n",
        "                image = transforms.functional.hflip(image)\n",
        "                density_map_img = transforms.functional.hflip(density_map_img)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert the density map to a torch tensor after augmentation\n",
        "        density_map = torch.from_numpy(np.array(density_map_img)).unsqueeze(0).float()\n",
        "\n",
        "        return image, density_map\n",
        "\n",
        "\n",
        "# Data preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to resize both images and density maps to a fixed size for batch processing.\n",
        "\n",
        "    Args:\n",
        "        batch (list of tuples): Each tuple contains an image tensor and its corresponding density map tensor.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Tensor, Tensor]: Batched and resized image and density map tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    images = [item[0] for item in batch]\n",
        "    density_maps = [item[1] for item in batch]\n",
        "\n",
        "    # Set target size for resizing\n",
        "    target_height, target_width = 512, 512\n",
        "\n",
        "    resized_images = []\n",
        "    resized_density_maps = []\n",
        "\n",
        "    for img, density_map in zip(images, density_maps):\n",
        "        # Resize image to target size\n",
        "        resized_img = transforms.functional.resize(img, (target_height, target_width))\n",
        "        resized_images.append(resized_img)\n",
        "\n",
        "        # Resize density map using bilinear interpolation (preserves smoothness)\n",
        "        resized_density_map = transforms.functional.resize(\n",
        "            density_map,\n",
        "            (target_height, target_width),\n",
        "            interpolation=transforms.InterpolationMode.BILINEAR\n",
        "        )\n",
        "\n",
        "        resized_density_maps.append(resized_density_map)\n",
        "\n",
        "    # Stack resized images and density maps into batches\n",
        "    images = torch.stack(resized_images, 0)\n",
        "    density_maps = torch.stack(resized_density_maps, 0)\n",
        "\n",
        "    return images, density_maps\n",
        "\n",
        "\n",
        "# Define DataLoaders for training and testing\n",
        "train_dataset = CrowdDataset(train_image_path, train_density_path, transform=transform, augment=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "test_dataset = CrowdDataset(test_image_path, test_density_path, transform=transform, augment=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)\n"
      ],
      "metadata": {
        "id": "gu310Ec0YyNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install optuna for hyperparameter tuning"
      ],
      "metadata": {
        "id": "cNjv46GQhy0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "d17vOSHdMVlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output for SGD optimizer"
      ],
      "metadata": {
        "id": "kJbzb3gYkfoV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "403bbcbf"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSRNet Architecture for Crowd Counting with Pretrained VGG-16 Frontend and Dilated Backend"
      ],
      "metadata": {
        "id": "-oGtdweolo6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The make_layers function builds convolutional layers based on the VGG-16 structure. The frontend uses pretrained VGG-16 layers for feature extraction, while the backend uses dilated convolutions to preserve spatial detail. The output layer generates a single-channel density map, with backend and output weights initialized randomly."
      ],
      "metadata": {
        "id": "-4Bkb5TLlof0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fc16032"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "def make_layers(cfg: list, in_channels: int = 3, batch_norm: bool = False, dilation: bool = False) -> nn.Sequential:\n",
        "    \"\"\"\n",
        "    Builds a sequence of convolutional and pooling layers based on the provided configuration.\n",
        "\n",
        "    Args:\n",
        "        cfg (list): List of layer specifications, integers indicate number of filters, 'M' indicates MaxPooling.\n",
        "        in_channels (int): Number of input channels. Default is 3 for RGB images.\n",
        "        batch_norm (bool): Unused here, placeholder for batch normalization support.\n",
        "        dilation (bool): Whether to use dilated convolutions in the backend.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Sequential model consisting of the specified layers.\n",
        "    \"\"\"\n",
        "    d_rate = 2 if dilation else 1  # Set dilation rate\n",
        "    layers = []\n",
        "\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            # Add MaxPooling layer\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            # Add convolutional layer with optional dilation\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]  # Add ReLU activation\n",
        "            in_channels = v  # Update input channels for next layer\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class CSRNet(nn.Module):\n",
        "    \"\"\"\n",
        "    CSRNet: A convolutional neural network for crowd counting using a VGG-16 based frontend.\n",
        "\n",
        "    The model consists of:\n",
        "    - A frontend based on VGG-16 layers for feature extraction.\n",
        "    - A backend using dilated convolutions for preserving spatial information.\n",
        "    - An output layer that produces the final density map.\n",
        "\n",
        "    The frontend is initialized with pretrained VGG-16 weights to leverage transfer learning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CSRNet, self).__init__()\n",
        "\n",
        "        # Configuration for frontend and backend layers\n",
        "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
        "        self.backend_feat = [512, 512, 512, 256, 128, 64]\n",
        "\n",
        "        # Build frontend and backend\n",
        "        self.frontend = make_layers(self.frontend_feat)\n",
        "        self.backend = make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
        "\n",
        "        # Final output layer to produce the density map (single-channel output)\n",
        "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "        # Load pretrained VGG-16 weights for frontend\n",
        "        vgg = models.vgg16(pretrained=True)\n",
        "\n",
        "        self._initialize_weights()  # Initialize weights for backend and output layer\n",
        "\n",
        "        # Transfer VGG-16 weights to frontend layers\n",
        "        frontend_items = list(self.frontend.state_dict().items())\n",
        "        vgg_items = list(vgg.state_dict().items())\n",
        "        for i in range(len(frontend_items)):\n",
        "            frontend_items[i][1].data[:] = vgg_items[i][1].data[:]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the CSRNet model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input image tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted density map.\n",
        "        \"\"\"\n",
        "        x = self.frontend(x)        # Feature extraction\n",
        "        x = self.backend(x)         # Contextual refinement using dilated convolutions\n",
        "        x = self.output_layer(x)    # Density map prediction\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes the weights of the backend and output layer using a normal distribution.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function evaluates the trained model by calculating the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) between the predicted and actual crowd counts. It sums the total error across all batches and averages it. The function returns MAE and RMSE as performance metrics for the model."
      ],
      "metadata": {
        "id": "GQBoAusVmRoD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c523163"
      },
      "source": [
        "def evaluate_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader) -> tuple:\n",
        "    \"\"\"\n",
        "    Evaluates the performance of the model on a given dataset using MAE and RMSE metrics.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained crowd counting model.\n",
        "        data_loader (torch.utils.data.DataLoader): DataLoader for the validation or test dataset.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) for the dataset.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    mae = 0.0  # Initialize Mean Absolute Error\n",
        "    mse = 0.0  # Initialize Mean Squared Error\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking for evaluation\n",
        "        for images, densities in data_loader:\n",
        "            images = images.to(device)\n",
        "            densities = densities.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Accumulate the absolute and squared differences between predicted and actual counts\n",
        "            mae += torch.abs(outputs.sum() - densities.sum()).item()\n",
        "            mse += ((outputs.sum() - densities.sum()) ** 2).item()\n",
        "\n",
        "    # Calculate average MAE and RMSE\n",
        "    mae /= len(data_loader)\n",
        "    rmse = (mse / len(data_loader)) ** 0.5\n",
        "\n",
        "    return mae, rmse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "This function trains the CSRNet model using either SGD or Adam optimizers based on the trial’s hyperparameters. It logs training loss, MAE, and RMSE to TensorBoard while handling NaN losses safely. The best-performing model (with the lowest MAE) is saved and returned after validation."
      ],
      "metadata": {
        "id": "DXhLI1_CmjwU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12ce8c64"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Tuple\n",
        "\n",
        "def get_writer(trial_number: int) -> SummaryWriter:\n",
        "    \"\"\"\n",
        "    Initializes and returns a TensorBoard SummaryWriter for the given trial.\n",
        "\n",
        "    Args:\n",
        "        trial_number (int): The trial number for logging.\n",
        "\n",
        "    Returns:\n",
        "        SummaryWriter: TensorBoard writer object.\n",
        "    \"\"\"\n",
        "    log_dir = f'/content/drive/MyDrive/crowd_density/runs/trial_{trial_number}'  # Save logs to Google Drive\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    return writer\n",
        "\n",
        "\n",
        "def train_model(train_loader: DataLoader, val_loader: DataLoader, lr: float, optimizer_name: str, num_epochs: int, trial_number: int) -> Tuple[float, torch.nn.Module]:\n",
        "    \"\"\"\n",
        "    Trains the CSRNet model using the specified hyperparameters and logs metrics to TensorBoard.\n",
        "\n",
        "    Args:\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
        "        lr (float): Learning rate.\n",
        "        optimizer_name (str): Optimizer to use ('SGD' or 'Adam').\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        trial_number (int): Current trial number for tracking experiments.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, torch.nn.Module]: The best MAE achieved and the trained model.\n",
        "    \"\"\"\n",
        "    model = CSRNet().to(device)\n",
        "    criterion = torch.nn.MSELoss(reduction='sum')  # Sum reduction to calculate total density error\n",
        "\n",
        "    # Select optimizer based on trial parameters\n",
        "    if optimizer_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.95)\n",
        "    else:  # Use Adam optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Initial evaluation to set the starting best MAE\n",
        "    initial_mae, _ = evaluate_model(model, val_loader)\n",
        "    best_mae = initial_mae\n",
        "    best_model_state_dict = model.state_dict().copy()  # Save initial model state\n",
        "\n",
        "    writer = get_writer(trial_number)  # TensorBoard writer\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0  # Track cumulative loss for the epoch\n",
        "\n",
        "        for batch_idx, (images, densities) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            densities = densities.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Resize outputs to match density map size\n",
        "            outputs_resized = F.interpolate(outputs, size=densities.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            loss = criterion(outputs_resized, densities)\n",
        "\n",
        "            # Handle NaN losses to avoid breaking training\n",
        "            if not torch.isnan(loss):\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "            else:\n",
        "                print(f\"Warning: NaN loss encountered in Trial {trial_number}, Epoch {epoch + 1}, Batch {batch_idx}. Skipping backpropagation.\")\n",
        "\n",
        "        # Evaluate the model on validation data after each epoch\n",
        "        mae, rmse = evaluate_model(model, val_loader)\n",
        "\n",
        "        print(f'Trial {trial_number}, Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.2f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}')\n",
        "\n",
        "        # Log training metrics to TensorBoard\n",
        "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
        "        writer.add_scalar('MAE/val', mae, epoch)\n",
        "        writer.add_scalar('RMSE/val', rmse, epoch)\n",
        "\n",
        "        # Save the best model based on lowest MAE\n",
        "        if mae < best_mae:\n",
        "            best_mae = mae\n",
        "            best_model_state_dict = model.state_dict().copy()\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    # Load best model weights before returning\n",
        "    model.load_state_dict(best_model_state_dict)\n",
        "    return best_mae, model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Optuna objective\n",
        "This function defines the Optuna objective, where learning rate and optimizer type are sampled to find the best hyperparameters for training. It trains the CSRNet model, saves the best model, evaluates it on the test set, and logs all trial details for future tracking. The returned MAE guides Optuna in minimizing the crowd counting error."
      ],
      "metadata": {
        "id": "iur_0-panGCS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "958ad8a0"
      },
      "source": [
        "import optuna\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from typing import Any\n",
        "\n",
        "def objective(trial: optuna.trial.Trial) -> float:\n",
        "    \"\"\"\n",
        "    Objective function for Optuna hyperparameter optimization.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): An Optuna trial object for sampling hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        float: The Mean Absolute Error (MAE) on the test set, which Optuna will minimize.\n",
        "    \"\"\"\n",
        "    # Suggest learning rate using log scale between 1e-7 and 1e-3\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True) # need to change the learning rate to 1e-5 as the current learning rate may raise NaN loss\n",
        "\n",
        "    # Only using Adam optimizer in this setup\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam'])\n",
        "\n",
        "    # Set number of epochs for this trial\n",
        "    num_epochs = 50  # Can be increased later for more robust training\n",
        "\n",
        "    # Train model and retrieve best MAE and trained model for this trial\n",
        "    best_mae_in_trial, trained_model = train_model(train_loader, test_loader, lr, optimizer_name, num_epochs, trial.number)\n",
        "\n",
        "    # Save the best model for this trial to Google Drive\n",
        "    best_model_path = f'/content/drive/MyDrive/crowd_density/best_model_trial_{trial.number}.pth'\n",
        "    torch.save(trained_model.state_dict(), best_model_path)\n",
        "\n",
        "    # Final evaluation on the test set\n",
        "    mae, rmse = evaluate_model(trained_model, test_loader)\n",
        "\n",
        "    # Log trial details for future reference\n",
        "    with open('/content/drive/MyDrive/crowd_density/optuna_best_trials.txt', 'a') as f:\n",
        "        f.write(f\"Trial {trial.number}:\\n\")\n",
        "        f.write(f\"  Value (MAE): {mae:.2f}\\n\")\n",
        "        f.write(f\"  Params: {trial.params}\\n\")\n",
        "        f.write(f\"  Best MAE during training: {best_mae_in_trial:.2f}\\n\")\n",
        "        f.write(f\"  Model path: {best_model_path}\\n\\n\")\n",
        "\n",
        "    return mae  # Return MAE as the objective to minimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optuna study for hyperparameter optimization\n",
        "This block initializes an Optuna study to minimize the Mean Absolute Error (MAE) by tuning hyperparameters. It runs the specified number of trials and automatically finds the best combination of hyperparameters. After completion, it prints the best trial's MAE and the corresponding hyperparameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "f4qPlm1nnUiP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "679d763a"
      },
      "source": [
        "import optuna\n",
        "\n",
        "# Create Optuna study for hyperparameter optimization\n",
        "study: optuna.study.Study = optuna.create_study(direction='minimize')  # We aim to minimize MAE\n",
        "\n",
        "# Run the optimization process with the specified number of trials\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=True)  # can increase n_trials for better tuning\n",
        "\n",
        "# Display the best trial results\n",
        "print(\"Best trial:\")\n",
        "print(f\"  Value: {study.best_trial.value:.2f} (MAE)\")  # Best MAE achieved across all trials\n",
        "print(\"  Params: \")\n",
        "for key, value in study.best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")  # Display the best hyperparameters found"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eaae668"
      },
      "source": [
        "### Visualize results with TensorBoard\n",
        "\n",
        "TensorBoard to visualize the training loss and validation MAE/RMSE for each trial. The logs are saved to Google Drive in the `crowd_density/runs` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e42e389d"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/crowd_density/runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crowd Count Prediction and Density Map Visualization\n",
        "\n",
        "This function loads a pre-trained CSRNet model and predicts the crowd count for a given input image. To prevent GPU memory overflow, the image is resized to a manageable size before inference. The function then visualizes both the original image and its corresponding predicted density map side-by-side, providing a clear comparison. The predicted crowd count is calculated by summing all the values in the density map, which serves as an approximation of the total number of people present in the image."
      ],
      "metadata": {
        "id": "dsIGx20tjjdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "def predict_image_with_density_map(image_path: str, model_path: str) -> int:\n",
        "    \"\"\"\n",
        "    Loads a trained CSRNet model, performs inference on a given image, and visualizes the predicted density map.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "        model_path (str): Path to the saved model weights.\n",
        "\n",
        "    Returns:\n",
        "        int: The predicted crowd count in the input image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the trained model\n",
        "    model = CSRNet().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Load and prepare the input image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Resize image to match the model’s expected input size to avoid GPU memory issues\n",
        "    target_height, target_width = 512, 512\n",
        "    resize_transform = transforms.Compose([\n",
        "        transforms.Resize((target_height, target_width)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Apply transformations and add batch dimension\n",
        "    img_transformed = resize_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Forward pass (inference) without gradient computation\n",
        "    with torch.no_grad():\n",
        "        output = model(img_transformed)\n",
        "\n",
        "    # The output is a density map with smaller spatial dimensions\n",
        "    predicted_count = int(output.sum().item())  # Sum of density map = estimated crowd count\n",
        "\n",
        "    # Convert the density map to a 2D NumPy array for visualization\n",
        "    density_map = output.squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Visualization: Original image and predicted density map side by side\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(density_map, cmap='jet')\n",
        "    plt.title(f'Predicted Density Map (Count: {predicted_count})')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_count"
      ],
      "metadata": {
        "id": "nZTF_gGtiZhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (update with your image path and best model path)\n",
        "test_image_path = '/content/drive/MyDrive/crowd_density/test_image1.jpg'\n",
        "best_model_path = '/content/drive/MyDrive/crowd_density/best_model_trial_1.pth' # Corrected path\n",
        "\n",
        "predicted_count = predict_image_with_density_map(test_image_path, best_model_path)\n",
        "print(f'Predicted Crowd Count: {predicted_count}')"
      ],
      "metadata": {
        "id": "jMOCsmeUj-Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (update with your image path and best model path)\n",
        "test_image_path = '/content/drive/MyDrive/crowd_density/test_image2.jpg'\n",
        "best_model_path = '/content/drive/MyDrive/crowd_density/best_model_trial_1.pth'\n",
        "\n",
        "predicted_count = predict_image_with_density_map(test_image_path, best_model_path)\n",
        "print(f'Predicted Crowd Count: {predicted_count}')"
      ],
      "metadata": {
        "id": "nPpBGutunKo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "test_image_path = '/content/drive/MyDrive/crowd_density/test_image3.jpg'\n",
        "best_model_path = '/content/drive/MyDrive/crowd_density/best_model_trial_1.pth'\n",
        "\n",
        "predicted_count = predict_image_with_density_map(test_image_path, best_model_path)\n",
        "print(f'Predicted Crowd Count: {predicted_count}')"
      ],
      "metadata": {
        "id": "1wIDaR_MnneS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}